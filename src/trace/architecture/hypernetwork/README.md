# Hypernetworks and Iterated Hypernetworks


We show that depth-n linear hypernetworks with linear weight-generating networks compute degree-n polynomials. We also argue that hypernetworks with weight-generating networks that have fully-connected final layers are as easier to train as ordinary neural networks. These arguments theoretically motivate a new class of highly expressive models called $\textit{iterated hypernetworks}$, which have weight-generating networks are also hypernetworks. We show that iterated hypernetworks with $n^k$ layers total can compute polynomials with degree up to $\exp(\exp\dots\exp(n,n),\dots n), n) (k \text{ times})$. We then evaluate the performance of iterated linear hyperneworks on nolinear regression tasks.